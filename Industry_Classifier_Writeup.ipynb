{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e46a87-280c-4b99-b1ca-9860d9199357",
   "metadata": {},
   "source": [
    "# Industry Fundamentals Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b0698-5592-4d4e-a19b-8b9fdbb76399",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The goal of this project was to build out a systematic way to classify companies on their fundamentals. This could serve two main functions: \n",
    "* Highlight interesting industry trends and anomalies \n",
    "* Compute most similar peers in systematic way outside of market price correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e65a13b-8530-4185-8df0-c280e9c9beb8",
   "metadata": {},
   "source": [
    "## Design and Data\n",
    "\n",
    "Although the entire sample set of companies that trade on the NYSE, AMEX, and NASDAQ are ~8,000, I chose the constitutents of the Russell 3000 to be my training, validation, and test data. The current Russel 3K market cap cutoff is at ~30mm, which provides a useful heuristic for companies with real financial data. I used the Yahoo Finance library to build my dataset on these 3,000 companies. This dataset had 8 final features: Market Cap, Dividend Rate, Price to Book, EV to Revenue, EV to EBITDA, Trailing EPS, Profit Margin, and Implied Volatility. These features were used to distinguish target category or the Yahoo Finance designated \"sector\", of which there are eleven: Basic Materials, Communication Services, Consumer Cyclical, Consumer Defensive, Energy, Financial Services, Healthcare, Industrials, Real Estate, Technology, and Utilities. \n",
    "\n",
    "After training my model, my predictive power per sector was as follows: \n",
    "\n",
    " ![title](recall_precision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410476aa-5fc5-4d7e-a0eb-05afc00c2d11",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "My process begai with assessing the type of classification algorithm that would work best with my data types. The XGBoost classifier had the best tradeoff of predictive power and flexibility with variable data types (given sporadic nulls throughout my dataset). After hyperparameter tuning, my final algorithm with parenters was: \n",
    "* XGBClassifier (n_estimators = 100, learning_rate=0.4, objective=\"reg:squarederror\", max_depth=3, subsample=1, gamma = 0, colsample_bytree=0.4)\n",
    "\n",
    "This algo left me with .54 accuracy for first choice prediction, .69 accuracy for its \"top 2\" accuracy, and the following confusion matrix on my test data: \n",
    "\n",
    " ![title](CF_Matrix2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b77ec-42f6-42eb-b129-0207136aedff",
   "metadata": {},
   "source": [
    "## Tools \n",
    "\n",
    "* Pandas, Numpy, SKlearn, XGBClassifier for data exploration and algorithms \n",
    "* Seaborn, Matplotlib for visualization\n",
    "* Streamlit for app presentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a29ea-8e74-46fb-a613-449d7cbbd916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
